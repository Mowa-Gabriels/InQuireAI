{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_huggingface.llms.huggingface_endpoint import HuggingFaceEndpoint # Now the import should work\n",
    "import os\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"data\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store the data as one variable(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen = \"\"\n",
    "for page in data:\n",
    "    question_gen += page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "text_chunk=text_splitter.split_documents(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the API key from the environment variables\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "\n",
    "# Check if the API key was found\n",
    "if HUGGINGFACEHUB_API_TOKEN:\n",
    "    print(\"API key found.\")\n",
    "    # Now you can use OPENAI_API_KEY in your code\n",
    "else:\n",
    "    print(\"API key not found in .env file.\")\n",
    "    HUGGINGFACEHUB_API_TOKEN = input(\"Please enter your HuggingFcae API token: \")\n",
    "    # You might want to store the entered key in the .env file for future use\n",
    "    # but be careful about security implications if you're sharing the file.\n",
    "    with open('.env', 'a') as f:\n",
    "        f.write(f'\\nHUGGINGFACEHUB_API_TOKEN=\"{HUGGINGFACEHUB_API_TOKEN}\"')\n",
    "    print(\"API key stored in .env file for future use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingfacehub_api_token = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    task='text-generation',\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.3,\n",
    "    huggingfacehub_api_token=huggingfacehub_api_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=llm.invoke(\"who is the owner of openAI\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "emb_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model.embed_documents(\"text_chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import embeddings\n",
    "from langchain_chroma import Chroma\n",
    "persist_directory = 'db'\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=text_chunk,\n",
    "                                 embedding=emb_model,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"tell me about SDG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=vectordb.similarity_search(query,k=5)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "Please analyze the conteXt and generate 20 diverse and insightful questions that could be asked about the information within the PDF. For each question, also provide a concise and accurate answer based on the PDF's content.\n",
    "\n",
    "\n",
    "\n",
    "Requirements:\n",
    "* Question types: Aim for a variety of question types (e.g., factual, inferential, definitional).\n",
    "* Relevance: All questions and answers must be directly related to the information presented in the PDF without using the word PDF.\n",
    "* Clarity: Questions and answers should be clearly worded and easy to understand.\n",
    "* Conciseness: Answers should be concise but provide sufficient information.\n",
    "* Accuracy: Answers must be factually correct according to the PDF.\n",
    "\n",
    "Output format:\n",
    "Present the questions and answers in a numbered list format, like this:\n",
    "\n",
    "Question: [Question 1]\n",
    "Answer: [Answer 1]\n",
    "\n",
    "\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\"]\n",
    ")\n",
    "\n",
    "# Update the RetrievalQA chain with the new prompt\n",
    "query_retriever_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_PROMPT} \n",
    ")\n",
    "\n",
    "# Run the query to generate questions and answers\n",
    "result = query_retriever_chain({\"query\": \"SDG\"}) \n",
    "qa_list = result[\"result\"].split(\"\\n\\n\")# Split into question-answer pairs\n",
    "\n",
    "for qa_pair in qa_list:\n",
    "    print(qa_pair + \"\\n-------------------------------------------\\n\")\n",
    "\n",
    "# Save th# Add line breakers and save to QnA.txt\n",
    "\n",
    "with open(\"QnA.txt\", \"w\") as f:\n",
    "    for qa_pair in qa_list:\n",
    "        f.write(qa_pair + \"\\n--------------------------------------------\\n\")  # Add line breaker after each paire result to QnA.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
